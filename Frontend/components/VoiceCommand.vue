<template>
  <div class="voice-command-container bg-white rounded-xl shadow-md p-4 w-full max-w-md">
    <!-- Visualizer -->
    <VoiceVisualizer :isRecording="isRecording" :decibelLevel="decibelLevel" />

    <!-- User's voice input -->
    <div 
      v-if="lastTranscript" 
      class="mt-3 p-2 bg-gray-50 rounded-lg border border-gray-100"
    >
      <p class="text-sm text-gray-600">
        <span class="font-medium">You said:</span> "{{ lastTranscript }}"
      </p>
    </div>

    <!-- Feedback message -->
    <div 
      v-if="feedback" 
      class="mt-3 p-2 rounded-lg flex items-center space-x-2"
      :class="feedbackType === 'success' ? 'bg-green-50 text-green-700' : feedbackType === 'error' ? 'bg-red-50 text-red-700' : 'bg-blue-50 text-blue-700'"
    >
      <i 
        :class="feedbackType === 'success' ? 'fas fa-check-circle' : feedbackType === 'error' ? 'fas fa-times-circle' : 'fas fa-info-circle'"
        class="text-sm"
      ></i>
      <p class="text-sm">{{ feedback }}</p>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, onUnmounted, computed } from 'vue';
import type { Ref } from 'vue';
import { useNuxtApp } from '#app';
import VoiceVisualizer from './VoiceVisualizer.vue';
import { useSpotifyStore } from '@/stores/spotify';
import { useNewsStore } from '@/stores/news';
import { useChatbotStore } from '@/stores/chatbot'


const chatbotStore = useChatbotStore()

const newsStore = useNewsStore();
const emit = defineEmits(['featureMatched']);
const { $sendAudioForCommand } = useNuxtApp();
const spotifyStore = useSpotifyStore();

const props = defineProps<{
  selectedFeature: string;
  cameraRef: Ref | null;
}>();

// State
const isRecording = ref(false);
const decibelLevel = ref(0);
const feedback = ref<string | null>(null);
const feedbackType = ref<'success' | 'error' | 'info'>('info');
const lastTranscript = ref<string | null>(null);

let mediaRecorder: MediaRecorder | null = null;
let audioChunks: Blob[] = [];
let audioContext: AudioContext | null = null;
let analyser: AnalyserNode | null = null;
let intervalId: ReturnType<typeof setInterval>;

// Constants
const DECIBEL_THRESHOLD = 10;
const SPEECH_MIN_DURATION = 500;
const FEEDBACK_DURATION = 5000; // How long feedback messages stay visible (ms)
let speechDetected = false;
let speechStartTime: number | null = null;
let feedbackTimeout: ReturnType<typeof setTimeout> | null = null;  // Corrected type for browser-based code

const sleep = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

const globalCommands = {
  stop: ['stop', 'mute', 'shut up', 'quiet'],
  help: ['help', 'what can i say', 'commands'],
  cancel: ['cancel', 'never mind'],
};

// Define available commands for each feature
const featureCommands = {
  'Text': {
    'read': ['read', 'read text', 'read aloud'],
    'translate': ['translate', 'translate text'],
    'save': ['save', 'save text', 'remember'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Currency': {
    'convert': ['convert', 'exchange', 'calculate'],
    'compare': ['compare', 'difference'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Object': {
    'identify': ['identify', 'what is this', 'object'],
    'describe': ['describe', 'details', 'tell me more'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Product': {
    'search': ['search', 'find similar', 'shop'],
    'price': ['price', 'how much', 'cost'],
    'reviews': ['reviews', 'ratings'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Distance': {
    'measure': ['measure', 'how far', 'distance'],
    'compare': ['compare', 'difference'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Face': {
    'identify': ['identify', 'who is this', 'recognize'],
    'remember': ['remember', 'save face', 'add person'],
    'capture': ['capture', 'take a picture', 'snapshot'],
  },
  'Music': {
    'detect': ['detect', 'what song', 'identify song', 'recognize'],
    'play': ['play', 'start', 'resume'],
    'pause': ['pause', 'stop', 'mute'],
  },
  'Chatbot': {
    'ask': ['ask', 'question', 'help me'],
    'chat': ['chat', 'talk', 'converse'],
  },
  'News': {
    'summarize': ['summarize', 'summary', 'brief'],
    'read': ['read', 'read aloud', 'narrate'],
  }
};

// Add this above setup block or with other functions
const cancelSpeech = () => {
  if (speechSynthesis.speaking) {
    speechSynthesis.cancel();
    console.log('ðŸ›‘ Speech synthesis canceled due to user speaking.');
  }
};

const getAvailableCommands = () => {
  const global = Object.keys(globalCommands).join(', ');
  if (!props.selectedFeature || !(featureCommands as Record<string, Record<string, string[]>>)[props.selectedFeature]) {
    return `Global: ${global}`;
  }

  const featureSpecific = Object.keys((featureCommands as Record<string, Record<string, string[]>>)[props.selectedFeature]).join(', ');
  return `Feature: ${featureSpecific} | Global: ${global}`;
};


//////////////// ARTICLE_SECTION //////////////////////

const ordinalMap: Record<string, number> = {
  first: 0,
  second: 1,
  third: 2,
  fourth: 3,
  fifth: 4,
  sixth: 5,
  seventh: 6,
  eighth: 7,
  ninth: 8,
  tenth: 9
};
const getArticleIndexFromCommand = (command: string): number | null => {
  const lower = command.toLowerCase();

  // Match "first", "second", etc.
  for (const key in ordinalMap) {
    console.log(key)
    if (lower.includes(key)) return ordinalMap[key];
  }

  // Match digits (e.g. "read article 3")
  const match = lower.match(/(?:article\s*)?(\d+)/);
  if (match && match[1]) {
    const num = parseInt(match[1]);
    if (!isNaN(num) && num >= 1 && num <= newsStore.articles.length) {
      return num - 1;
    }
  }

  return null;
};

const findArticleByQuery = (query: string) => {
  const lowerQuery = query.toLowerCase();
  return newsStore.articles.find(article =>
    article.title.toLowerCase().includes(lowerQuery) ||
    article.summary?.toLowerCase().includes(lowerQuery)
  );
};

const speakArticle = async (article: { title: string; summary: string }) => {
  const utterance = new SpeechSynthesisUtterance(`${article.title}. ${article.summary}`);
  utterance.lang = 'en-US';
  speechSynthesis.speak(utterance);
};

/////////////////////////////////////////////////////

// Start voice recording
const startRecording = async () => {
  // If recording is already active, do nothing
  if (isRecording.value) return;

  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioChunks = [];

    // Set up audio analysis - handle possible existing audioContext
    if (audioContext && audioContext.state === 'suspended') {
      await audioContext.resume();
    } else {
      audioContext = new AudioContext();
    }

    analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    analyser.fftSize = 256;
    source.connect(analyser);

    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    // Track audio levels for visualization and speech detection
    const trackDecibels = () => {
      if (!analyser || !isRecording.value) return;

      analyser.getByteTimeDomainData(dataArray);
      let sum = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const normalized = dataArray[i] - 128;
        sum += normalized * normalized;
      }
      const rms = Math.sqrt(sum / dataArray.length);
      decibelLevel.value = Math.min(100, rms * 10);

      const now = Date.now();
      if (decibelLevel.value > DECIBEL_THRESHOLD) {
        // cancelSpeech(); // ðŸ’¥ Interrupt system speech when user starts speaking
        if (!speechStartTime) speechStartTime = now;
        else if (now - speechStartTime > SPEECH_MIN_DURATION) {
          speechDetected = true;
        }
      } else {
        speechStartTime = null;
      }

      if (isRecording.value) requestAnimationFrame(trackDecibels);
    };

    // Set up media recorder
    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);

    mediaRecorder.onstop = async () => {

      // Don't process if we manually stopped recording for music detection
      if (!isRecording.value) {
        console.log('Recording stopped manually. Skipping processing.');
        resetSpeechFlags();
        return;
      }

      if(speechSynthesis.speaking) {
        console.log('Feedback is being spoken. Skipping request.');
        resetSpeechFlags();
        return;
      }

      if (!speechDetected) {
        console.log('âŒ No significant speech detected. Skipping request.');
        resetSpeechFlags();
        return;
      }

      // Process the audio for commands
      const blob = new Blob(audioChunks, { type: 'audio/webm' });
      const data = await $sendAudioForCommand(props.selectedFeature, blob);


      const commandText = data.command;

      lastTranscript.value = data.query;

      if (data.query === "") {
        console.log("Nothing to record")
        resetSpeechFlags();
        return;
      }

      // if the current feature matched the command then it is a query of the feature
      if (props.selectedFeature === data.command && data.command === "News") {
        if (data.intent === "read" || commandText.toLowerCase().includes("read")) {
          const articleIndex = getArticleIndexFromCommand(data.query);
          if (articleIndex !== null && newsStore.articles[articleIndex]) {
            const article = newsStore.articles[articleIndex];
            showFeedback(`Reading article ${articleIndex + 1}...`);
            await speakArticle(article);
          } else {
            showFeedback("Sorry, I couldn't find which article you wanted me to read.");
          }
          return;
        }

        // Normal query intent
        if (data.intent === "query" && data.command === "News") {
          showFeedback("Fetching required articles...");
          await newsStore.fetchNews(data.query);
          showFeedback("Articles updated.");
          return;
        }
      }

      if (props.selectedFeature === data.command && data.command === "Chatbot") {
        if (data.intent == "query") {
          console.log("Chatbot query detected:", data.query);
          const reply = await chatbotStore.sendMessage(data.query);
          showFeedback(`${reply}`);
          return;
        }
      }

      if (!commandText) {
        showFeedback("Sorry, I didn't understand that.");
        resetSpeechFlags();
        return;
      }

      const lowerCommand = commandText.toLowerCase();
      console.log('Voice command recognized:', lowerCommand);

      // Check if command is a feature selection
      const featureMatch = Object.keys(featureCommands).find(
        feature => feature.toLowerCase() === lowerCommand
      );

      console.log('Feature match:', featureMatch);
      console.log('Selected feature:', props.selectedFeature);
      console.log("cameraRef:", props.cameraRef);

      if (featureMatch) {
        // User is selecting a feature
        showFeedback(`Switched to ${featureMatch} mode`);
        emit('featureMatched', featureMatch);
      }
      else if (props.selectedFeature) {
        // User is using a feature-specific command
        await handleFeatureCommand(lowerCommand);
      }
      else {
        // No feature selected yet
        showFeedback(`Please select a feature first. Say one of: ${Object.keys(featureCommands).join(', ')}`);
      }

      resetSpeechFlags();
    };

    const resetSpeechFlags = () => {
      speechDetected = false;
      speechStartTime = null;
      audioChunks = [];
    };

    // Start recording and analysis
    isRecording.value = true;
    mediaRecorder.start();
    trackDecibels();

    // Restart recording every 5 seconds
    intervalId = setInterval(() => {
      if (mediaRecorder?.state === 'recording') {
        mediaRecorder.stop();
        mediaRecorder.start();
      }
    }, 5000);
  } catch (error) {
    console.error('Error starting voice recording:', error);
    showFeedback('Unable to access microphone. Please check permissions.');
  }
};

const handleFeatureCommand = async (command: string) => {
  // Check for global commands first
  for (const action in globalCommands) {
    if ((globalCommands as Record<string, string[]>)[action].some((phrase: string) => command.includes(phrase))) {
      await executeGlobalCommand(action);
      return;
    }
  }

  const currentFeature = props.selectedFeature;
  if (!currentFeature || !(featureCommands as Record<string, Record<string, string[]>>)[currentFeature]) {
    showFeedback(`The command "${command}" doesn't match any available feature.`);
    return;
  }

  const commands = (featureCommands as Record<string, Record<string, string[]>>)[currentFeature];
  let matchedAction = null;

  for (const action in commands) {
    if ((commands as Record<string, string[]>)[action].some((phrase: string) => command.includes(phrase))) {
      matchedAction = action;
      break;
    }
  }

  if (matchedAction) {
    await executeCommand(currentFeature, matchedAction, command);
  } else {
    showFeedback(`The command "${command}" is not valid for ${currentFeature} mode. Try: ${getAvailableCommands()}`);
  }
};

const executeGlobalCommand = async (action: string) => {
  switch (action) {
    case 'stop':
      showFeedback('Stopping all audio...');
      speechSynthesis.cancel();
      if (spotifyStore.isPlaying) {
        spotifyStore.pauseMusic();
      }
      break;

    case 'help':
      showFeedback(`Say one of these global commands: ${Object.keys(globalCommands).join(', ')}`);
      break;

    case 'cancel':
      showFeedback('Okay, canceled.');
      break;

    default:
      showFeedback(`Unknown global command: ${action}`);
  }
};



// Execute the matched command
const executeCommand = async (feature: string, action: string, originalCommand: string) => {
  // showFeedback(`Executing: ${action} for ${feature}`);

  console.log(`Executing command: ${action} for feature: ${feature}`);

  switch (feature) {
    case 'Music':
      await executeMusicCommand(action);
      break;
    case 'Text':
    case 'Currency':
    case 'Object':
    case 'Product':
    case 'Distance':
    case 'Face':
      await executeCameraCommand(feature);
      break;
    case 'Chatbot':
      await executeChatbotCommand(action);
      break;
    case 'News':
      await executeNewsCommand(action);
      break;
    default:
      showFeedback(`Feature ${feature} not yet implemented`);
  }
};

// Execute music-specific commands
const executeMusicCommand = async (action: string) => {
  switch (action) {
    case 'detect':
      showFeedback('Detecting music');

      // Stop the microphone before music detection
      stopRecording();

      try {
        await spotifyStore.detectMusic();
        console.log("Music detected:", spotifyStore.currentTrack);
      } catch (error) {
        showFeedback('Failed to detect music. Please try again.');
        console.error('Music detection error:', error);
      } finally {
        // Restart the microphone after music detection is complete
        startRecording();
      }
      break;

    case 'play':
      showFeedback('Playing music');
      if (spotifyStore.currentTrack) {
        spotifyStore.playMusic();
      } else {
        showFeedback('No music is currently loaded.');
      }
      break;

    case 'pause':
      showFeedback('Pausing music');
      if (spotifyStore.isPlaying) {
        spotifyStore.pauseMusic();
      } else {
        showFeedback('Music is already paused.');
      }
      break;
  }
};

const executeCameraCommand = async (action: string) => {
  console.log("Taking a snapshot with current feature:", action)
  if (props.cameraRef && props.cameraRef.value) {
    await props.cameraRef.value.takeSnapshot() // Dereference the Ref
    showFeedback(`Capturing snapshot for ${action}...`)
  } else {
    showFeedback(`Camera not ready.`)
  }
}

const executeChatbotCommand = async (action: string) => {
  showFeedback(`Executing ${action} for chatbot feature`);
  // Implementation would depend on your chatbot feature capabilities
};

const executeNewsCommand = async (action: string) => {
  showFeedback(`Executing ${action} for News feature`);
  // Implementation would depend on your News feature capabilities
};

// Display feedback and speak it
const showFeedback = async (message: string, type: 'success' | 'error' | 'info' = 'info', delayAfter: number = 500) => {
  feedback.value = message;
  feedbackType.value = type;

  console.log('Feedback:', message);

  await speak(message);
  await sleep(delayAfter);

  if (feedbackTimeout) clearTimeout(feedbackTimeout);
  feedbackTimeout = setTimeout(() => {
    feedback.value = null;
  }, FEEDBACK_DURATION);
};

// Text-to-speech
const speak = (text: string): Promise<void> => {
  return new Promise(resolve => {
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.onend = () => resolve();
    speechSynthesis.speak(utterance);
  });
};


const stopRecording = async () => {
  isRecording.value = false;

  // Stop the recorder if active
  if (mediaRecorder && mediaRecorder.state === 'recording') {
    mediaRecorder.stop();
  }

  // Stop refreshing audio chunks
  clearInterval(intervalId);

  // Suspend audio context
  if (audioContext) {
    await audioContext.suspend(); // Wait for suspend to complete
  }

  // Release media tracks
  if (mediaRecorder?.stream) {
    mediaRecorder.stream.getTracks().forEach(track => track.stop());
  }

  // Reset everything
  audioChunks = [];
  mediaRecorder = null;
};

// Lifecycle hooks
onMounted(() => {
  startRecording();
});

onUnmounted(() => {
  stopRecording();
  if (feedbackTimeout) {
    clearTimeout(feedbackTimeout);
  }
});
</script>

<style scoped>
.voice-command-container {
  border-radius: 12px;
  transition: all 0.3s ease;
}
</style>